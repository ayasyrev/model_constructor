# AUTOGENERATED! DO NOT EDIT! File to edit: 00_constructor.ipynb (unless otherwise specified).

__all__ = ['ConvLayer', 'Flatten', 'Noop', 'Stem', 'BasicBlock', 'Body', 'Head', 'init_model', 'Net', 'resnet18',
           'resnet34']

# Cell
import torch.nn as nn
import torch
from collections import OrderedDict

# Cell
_act_fn = nn.ReLU(inplace=True)

class ConvLayer(nn.Sequential):
    """Basic conv layers block"""
    def __init__(self, ni, nf, ks=3, stride=1,
            act=True,  act_fn=_act_fn,
            bn_layer=True, bn_1st=False, zero_bn=False,
            padding=None, bias=True, groups=1):

        self.act = act
        if padding==None: padding = ks//2
        layers = [('conv', nn.Conv2d(ni, nf, ks, stride=stride, padding=padding, bias=bias, groups=groups))]
        act_bn = [('act_fn', act_fn)] if act else []
        if bn_layer:
            bn = nn.BatchNorm2d(nf)
            nn.init.constant_(bn.weight, 0. if zero_bn else 1.)
            act_bn += [('bn', bn)]
        if bn_1st: act_bn.reverse()
        layers += act_bn
        super().__init__(OrderedDict(layers))

# Cell
class Flatten(nn.Module):
    '''flat x to vector'''
    def __init__(self):
        super().__init__()
    def forward(self, x): return x.view(x.size(0), -1)

# Cell
class Noop(nn.Module): # alternative name Merge
    '''Dummy module for vizualize skip conn'''
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return x

# Cell
class Stem(nn.Sequential):
    """Base stem"""
    def __init__(self, c_in=3,  sizes=[], stem_out=64,
            use_bn=False, pool=True,
            bn_1st=False,  stride_on=0, **kwargs):
        self.sizes = [c_in] + sizes + [stem_out]
        num_layers = len(self.sizes)-1
        stem = [(f"conv{i}", ConvLayer(self.sizes[i], self.sizes[i+1],
                stride=2 if i==stride_on else 1, act=True,
                bn_layer=not use_bn if i==num_layers-1 else True, **kwargs
                )) for i in range(num_layers)]
        if pool: stem += [('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))]
        if use_bn: stem.append(('bn', nn.BatchNorm2d(stem_out)))
        super().__init__(OrderedDict(stem))
    def extra_repr(self):
            return f"sizes: {self.sizes}"

# Cell
class BasicBlock(nn.Module):
    """Basic block (simplified) as in pytorch resnet"""
    expansion = 1
    def __init__(self, ni, nf, stride=1, bn_1st=False, zero_bn=False,
#                  groups=1, base_width=64, dilation=1, norm_layer=None
                **kwargs):
        super().__init__()
        self.downsample = not ni==nf or stride==2
        self.conv = nn.Sequential(OrderedDict([
            ('conv_0', ConvLayer(ni, nf, stride=stride, bn_1st=bn_1st, **kwargs)),
            ('conv_1', ConvLayer(nf, nf, zero_bn=zero_bn, bn_1st=bn_1st, **kwargs))]))
        if self.downsample:
            self.downsample = ConvLayer(ni, nf, ks=1, stride=stride, act=False, **kwargs)
        self.merge = Noop()
        self.act_conn = _act_fn

    def forward(self, x):
        identity = x
        out = self.conv(x)
        if self.downsample:
            identity = self.downsample(x)
        return self.act_conn(self.merge(out + identity))

# Cell
class Body(nn.Sequential):
    '''Constructor for body'''
    def __init__(self, block,
                 body_in=64, body_out=512,
                 layer_szs=[64,128,256,], blocks=[2,2,2,2],
                 expansion=1, **kwargs):  # Downsample Module as parameter
        layer_szs = [body_in] + layer_szs + [body_out]
        num_layers = len(layer_szs)-1
        layers = [(f"layer_{i}", self._make_layer(block, layer_szs[i], layer_szs[i+1], blocks[i], 1 if i==0 else 2, **kwargs))
                    for i in range(num_layers)]
        super().__init__(OrderedDict(layers))
    def _make_layer(self, block, ni, nf, blocks, stride, **kwargs):
        return nn.Sequential(OrderedDict(
            [(f'block_{i}', block(ni if i==0 else nf, nf, stride if i==0 else 1, **kwargs))
              for i in range(blocks)]))


# Cell
class Head(nn.Sequential):
    '''base head'''
    def __init__(self, ni, nf, **kwargs):
        super().__init__(OrderedDict(
            [('pool', nn.AdaptiveAvgPool2d((1, 1))),
             ('flat', Flatten()),
             ('fc',   nn.Linear(ni, nf)),
             ]))

# Cell
def init_model(model, zero_bn=False):
    '''Init model'''
    for m in model.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

# Cell
class Net(nn.Sequential):
    '''Constructor for model'''
    def __init__(self,  stem=Stem,
                 body=Body, block=BasicBlock,
                 layer_szs=[64,128,256,], blocks=[2,2,2,2],
                 head=Head,
                 c_in=3,  num_classes=1000,
                 body_in=64, body_out=512, expansion=1,
                 bn_1st=False,
                init_type='normal', **kwargs):
        # c_in = 3
        # block_szs  = [64,128,128,256,256,512]
        super().__init__(OrderedDict([
            ('stem', stem(c_in=c_in,stem_out=body_in, **kwargs)),
            ('body', body(block, body_in, body_out, layer_szs=layer_szs, blocks=blocks, **kwargs)),
            ('head', head(body_out*expansion, num_classes, **kwargs))
            ]))
        init_model(self)

# Cell
def resnet18(**kwargs):
    """Constructs a ResNet-18 model. """
    return Net(block=BasicBlock, blocks=[2, 2, 2, 2], **kwargs)
def resnet34(**kwargs):
    """Constructs a ResNet-18 model. """
    return Net(block=BasicBlock, blocks=[3, 4, 6, 3], **kwargs)