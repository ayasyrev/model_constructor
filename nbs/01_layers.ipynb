{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells will be exported to model_constructor.layers,\n",
      "unless a different module is specified after an export flag: `%nbdev_export special.module`\n"
     ]
    }
   ],
   "source": [
    "#default_exp layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "> Basic layers for constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.utils import spectral_norm\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Flatten(nn.Module):\n",
    "    '''flat x to vector'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def noop(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "class Noop(nn.Module):\n",
    "    '''Dummy module'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "class ConvLayer(nn.Sequential):\n",
    "    \"\"\"Basic conv layers block\"\"\"\n",
    "    Conv2d = nn.Conv2d\n",
    "\n",
    "    def __init__(self, ni, nf, ks=3, stride=1,\n",
    "                 act=True, act_fn=act_fn,\n",
    "                 bn_layer=True, bn_1st=True, zero_bn=False,\n",
    "                 padding=None, bias=False, groups=1, **kwargs):\n",
    "\n",
    "        if padding is None:\n",
    "            padding = ks // 2\n",
    "        layers = [('conv', self.Conv2d(ni, nf, ks, stride=stride, padding=padding, bias=bias, groups=groups))]\n",
    "        act_bn = [('act_fn', act_fn)] if act else []\n",
    "        if bn_layer:\n",
    "            bn = nn.BatchNorm2d(nf)\n",
    "            nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "            act_bn += [('bn', bn)]\n",
    "        if bn_1st:\n",
    "            act_bn.reverse()\n",
    "        layers += act_bn\n",
    "        super().__init__(OrderedDict(layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLayer(\n",
       "  (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act_fn): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = ConvLayer(32, 64)\n",
    "conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLayer(\n",
       "  (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = ConvLayer(32, 64, act=False)\n",
    "conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLayer(\n",
       "  (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (act_fn): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = ConvLayer(32, 64, bn_layer=False)\n",
    "conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLayer(\n",
       "  (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act_fn): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = ConvLayer(32, 64, bn_1st=True)\n",
    "conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLayer(\n",
       "  (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act_fn): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = ConvLayer(32, 64, bn_1st=True, act_fn=nn.LeakyReLU())\n",
    "conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "bs = 8\n",
    "xb = torch.randn(bs, 32, 32, 32)\n",
    "y = conv_layer(xb)\n",
    "y.shape\n",
    "assert y.shape == torch.Size([bs, 64, 32, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# SA module from mxresnet at fastai. todo - add persons!!!\n",
    "# Unmodified from https://github.com/fastai/fastai/blob/5c51f9eabf76853a89a9bc5741804d2ed4407e49/fastai/layers.py\n",
    "def conv1d(ni: int, no: int, ks: int = 1, stride: int = 1, padding: int = 0, bias: bool = False):\n",
    "    \"Create and initialize a `nn.Conv1d` layer with spectral normalization.\"\n",
    "    conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
    "    nn.init.kaiming_normal_(conv.weight)\n",
    "    if bias:\n",
    "        conv.bias.data.zero_()\n",
    "    return spectral_norm(conv)\n",
    "\n",
    "\n",
    "# Adapted from SelfAttention layer at\n",
    "# https://github.com/fastai/fastai/blob/5c51f9eabf76853a89a9bc5741804d2ed4407e49/fastai/layers.py\n",
    "# Inspired by https://arxiv.org/pdf/1805.08318.pdf\n",
    "class SimpleSelfAttention(nn.Module):\n",
    "    def __init__(self, n_in: int, ks=1, sym=False):  # , n_out:int):\n",
    "        super().__init__()\n",
    "        self.conv = conv1d(n_in, n_in, ks, padding=ks // 2, bias=False)\n",
    "        self.gamma = nn.Parameter(torch.tensor([0.]))\n",
    "        self.sym = sym\n",
    "        self.n_in = n_in\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.sym:\n",
    "            # symmetry hack by https://github.com/mgrankin\n",
    "            c = self.conv.weight.view(self.n_in, self.n_in)\n",
    "            c = (c + c.t()) / 2\n",
    "            self.conv.weight = c.view(self.n_in, self.n_in, 1)\n",
    "        size = x.size()\n",
    "        x = x.view(*size[:2], -1)   # (C,N)\n",
    "        # changed the order of mutiplication to avoid O(N^2) complexity\n",
    "        # (x*xT)*(W*x) instead of (x*(xT*(W*x)))\n",
    "        convx = self.conv(x)   # (C,C) * (C,N) = (C,N)   => O(NC^2)\n",
    "        xxT = torch.bmm(x, x.permute(0, 2, 1).contiguous())   # (C,N) * (N,C) = (C,C)   => O(NC^2)\n",
    "        o = torch.bmm(xxT, convx)   # (C,C) * (C,N) = (C,N)   => O(NC^2)\n",
    "        o = self.gamma * o + x\n",
    "        return o.view(*size).contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SE Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SEBlock(nn.Module):\n",
    "    \"se block\"\n",
    "    se_layer = nn.Linear\n",
    "    act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "    def __init__(self, c, r=16):\n",
    "        super().__init__()\n",
    "        ch = c // r\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            OrderedDict([('fc_reduce', self.se_layer(c, ch, bias=False)),\n",
    "                         ('se_act', self.act_fn),\n",
    "                         ('fc_expand', self.se_layer(ch, c, bias=False)),\n",
    "                         ('sigmoid', nn.Sigmoid())\n",
    "                         ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, _, _ = x.shape\n",
    "        y = self.squeeze(x).view(bs, c)\n",
    "        y = self.excitation(y).view(bs, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#         return torch.mul(x, y.expand_as(x))# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SEBlock(\n",
       "  (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "  (excitation): Sequential(\n",
       "    (fc_reduce): Linear(in_features=128, out_features=8, bias=False)\n",
       "    (se_act): ReLU(inplace=True)\n",
       "    (fc_expand): Linear(in_features=8, out_features=128, bias=False)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_block = SEBlock(128)\n",
    "se_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "bs = 8\n",
    "xb = torch.randn(bs, 128, 32, 32)\n",
    "y = se_block(xb)\n",
    "print(y.shape)\n",
    "assert y.shape == torch.Size([bs, 128, 32, 32]), f\"size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEBlockConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SEBlockConv(nn.Module):\n",
    "    \"se block with conv on excitation\"\n",
    "    se_layer = nn.Conv2d\n",
    "    act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "    def __init__(self, c, r=16):\n",
    "        super().__init__()\n",
    "#         c_in = math.ceil(c//r/8)*8\n",
    "        c_in = c // r\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('conv_reduce', self.se_layer(c, c_in, 1, bias=False)),\n",
    "                ('se_act', self.act_fn),\n",
    "                ('conv_expand', self.se_layer(c_in, c, 1, bias=False)),\n",
    "                ('sigmoid', nn.Sigmoid())\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.squeeze(x)\n",
    "        y = self.excitation(y)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SEBlockConv(\n",
       "  (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
       "  (excitation): Sequential(\n",
       "    (conv_reduce): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (se_act): ReLU(inplace=True)\n",
       "    (conv_expand): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_block = SEBlockConv(128)\n",
    "se_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "bs = 8\n",
    "xb = torch.randn(bs, 128, 32, 32)\n",
    "y = se_block(xb)\n",
    "print(y.shape)\n",
    "assert y.shape == torch.Size([bs, 128, 32, 32]), f\"size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end\n",
    "model_constructor\n",
    "by ayasyrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%nbdev_hide` not found.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
