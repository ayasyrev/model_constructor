{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells will be exported to model_constructor.activations,\n",
      "unless a different module is specified after an export flag: `%nbdev_export special.module`\n"
     ]
    }
   ],
   "source": [
    "#default_exp activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations functions.\n",
    "\n",
    "> Activations functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions, forked from https://github.com/rwightman/pytorch-image-models/timm/models/layers/activations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mish: Self Regularized  \n",
    "Non-Monotonic Activation Function  \n",
    "https://github.com/digantamisra98/Mish  \n",
    "fastai forum discussion https://forums.fast.ai/t/meet-mish-new-activation-function-possible-successor-to-relu  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# forked from https://github.com/rwightman/pytorch-image-models/timm/models/layers/activations.py\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def swish(x, inplace: bool = False):\n",
    "    \"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    return x.mul_(x.sigmoid()) if inplace else x.mul(x.sigmoid())\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(Swish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return swish(x, self.inplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SwishJit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.jit.script\n",
    "def swish_jit(x, inplace: bool = False):\n",
    "    \"\"\"Jit version of Swish.\n",
    "    Swish- Described in: https://arxiv.org/abs/1710.05941\n",
    "    \"\"\"\n",
    "    return x.mul(x.sigmoid())\n",
    "\n",
    "class SwishJit(nn.Module):\n",
    "    \"\"\"Jit version of Swish.\n",
    "    Swish - Described in: https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(SwishJit, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return swish_jit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SwishJitMe - memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@torch.jit.script\n",
    "def swish_jit_bwd(x, grad_output):\n",
    "    x_sigmoid = torch.sigmoid(x)\n",
    "    return grad_output * (x_sigmoid * (1 + x * (1 - x_sigmoid)))\n",
    "\n",
    "\n",
    "class SwishJitAutoFn(torch.autograd.Function):\n",
    "    \"\"\" torch.jit.script optimised Swish w/ memory-efficient checkpoint\n",
    "    Inspired by conversation btw Jeremy Howard & Adam Pazske\n",
    "    https://twitter.com/jeremyphoward/status/1188251041835315200\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return swish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        return swish_jit_bwd(x, grad_output)\n",
    "\n",
    "\n",
    "def swish_me(x, inplace=False):\n",
    "    return SwishJitAutoFn.apply(x)\n",
    "\n",
    "\n",
    "class SwishMe(nn.Module):\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(SwishMe, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return SwishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mish(x, inplace: bool = False):\n",
    "    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    NOTE: I don't have a working inplace variant\n",
    "    \"\"\"\n",
    "    return x.mul(F.softplus(x).tanh())\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        \"\"\"NOTE: inplace variant not working \"\"\"\n",
    "        super(Mish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return mish(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MishJit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.jit.script\n",
    "def mish_jit(x, _inplace: bool = False):\n",
    "    \"\"\"Jit version of Mish.\n",
    "    Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    \"\"\"\n",
    "    return x.mul(F.softplus(x).tanh())\n",
    "\n",
    "class MishJit(nn.Module):\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        \"\"\"Jit version of Mish.\n",
    "        Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\"\"\"\n",
    "        super(MishJit, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return mish_jit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MishJitMe - memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@torch.jit.script\n",
    "def mish_jit_fwd(x):\n",
    "#     return x.mul(torch.tanh(F.softplus(x)))\n",
    "    return x.mul(F.softplus(x).tanh())\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def mish_jit_bwd(x, grad_output):\n",
    "    x_sigmoid = torch.sigmoid(x)\n",
    "    x_tanh_sp = F.softplus(x).tanh()\n",
    "    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n",
    "\n",
    "\n",
    "class MishJitAutoFn(torch.autograd.Function):\n",
    "    \"\"\" Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    A memory efficient, jit scripted variant of Mish\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return mish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        return mish_jit_bwd(x, grad_output)\n",
    "\n",
    "\n",
    "def mish_me(x, inplace=False):\n",
    "    return MishJitAutoFn.apply(x)\n",
    "\n",
    "\n",
    "class MishMe(nn.Module):\n",
    "    \"\"\" Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    A memory efficient, jit scripted variant of Mish\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(MishMe, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return MishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardSwish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hard_swish(x, inplace: bool = False):\n",
    "    \"\"\"Hard swish activation function\"\"\"\n",
    "    inner = F.relu6(x + 3.).div_(6.)\n",
    "    return x.mul_(inner) if inplace else x.mul(inner)\n",
    "\n",
    "\n",
    "class HardSwish(nn.Module):\n",
    "    \"\"\"Hard swish activation function\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardSwish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return hard_swish(x, self.inplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardSwishJit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.jit.script\n",
    "def hard_swish_jit(x, inplace: bool = False):\n",
    "    # return x * (F.relu6(x + 3.) / 6)\n",
    "    return x * (x + 3).clamp(min=0, max=6).div(6.)  # clamp seems ever so slightly faster?\n",
    "\n",
    "\n",
    "class HardSwishJit(nn.Module):\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardSwishJit, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return hard_swish_jit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardSwishJitMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@torch.jit.script\n",
    "def hard_swish_jit_fwd(x):\n",
    "    return x * (x + 3).clamp(min=0, max=6).div(6.)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def hard_swish_jit_bwd(x, grad_output):\n",
    "    m = torch.ones_like(x) * (x >= 3.)\n",
    "    m = torch.where((x >= -3.) & (x <= 3.),  x / 3. + .5, m)\n",
    "    return grad_output * m\n",
    "\n",
    "\n",
    "class HardSwishJitAutoFn(torch.autograd.Function):\n",
    "    \"\"\"A memory efficient, jit-scripted HardSwish activation\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return hard_swish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        return hard_swish_jit_bwd(x, grad_output)\n",
    "\n",
    "\n",
    "def hard_swish_me(x, inplace=False):\n",
    "    \"\"\"A memory efficient, jit-scripted HardSwish activation\"\"\"\n",
    "    return HardSwishJitAutoFn.apply(x)\n",
    "\n",
    "\n",
    "class HardSwishMe(nn.Module):\n",
    "    \"\"\"A memory efficient, jit-scripted HardSwish activation\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardSwishMe, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return HardSwishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardMish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hard_mish(x, inplace: bool = False):\n",
    "    \"\"\" Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    if inplace:\n",
    "        return x.mul_(0.5 * (x + 2).clamp(min=0, max=2))\n",
    "    else:\n",
    "        return 0.5 * x * (x + 2).clamp(min=0, max=2)\n",
    "\n",
    "\n",
    "class HardMish(nn.Module):\n",
    "    \"\"\"Hard Mish, Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardMish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return hard_mish(x, self.inplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardMishJit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.jit.script\n",
    "def hard_mish_jit(x, inplace: bool = False):\n",
    "    \"\"\" Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (x + 2).clamp(min=0, max=2)\n",
    "\n",
    "\n",
    "class HardMishJit(nn.Module):\n",
    "    \"\"\" Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardMishJit, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return hard_mish_jit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardMishJitMe - memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.jit.script\n",
    "def hard_mish_jit_fwd(x):\n",
    "    return 0.5 * x * (x + 2).clamp(min=0, max=2)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def hard_mish_jit_bwd(x, grad_output):\n",
    "    m = torch.ones_like(x) * (x >= -2.)\n",
    "    m = torch.where((x >= -2.) & (x <= 0.), x + 1., m)\n",
    "    return grad_output * m\n",
    "\n",
    "\n",
    "class HardMishJitAutoFn(torch.autograd.Function):\n",
    "    \"\"\" A memory efficient, jit scripted variant of Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return hard_mish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        return hard_mish_jit_bwd(x, grad_output)\n",
    "\n",
    "\n",
    "def hard_mish_me(x, inplace: bool = False):\n",
    "    return HardMishJitAutoFn.apply(x)\n",
    "\n",
    "\n",
    "class HardMishMe(nn.Module):\n",
    "    \"\"\" A memory efficient, jit scripted variant of Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardMishMe, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return HardMishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "act_fn = Swish(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "act_fn = Mish(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end\n",
    "model_constructor\n",
    "by ayasyrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_Net.ipynb.\n",
      "Converted 01_activations.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 03_MXResNet.ipynb.\n",
      "Converted 04_YaResNet.ipynb.\n",
      "Converted 05_Twist.ipynb.\n",
      "Converted 10_base_constructor.ipynb.\n",
      "Converted 11_xresnet.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
