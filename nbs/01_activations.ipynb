{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells will be exported to model_constructor.activations,\n",
      "unless a different module is specified after an export flag: `%nbdev_export special.module`\n"
     ]
    }
   ],
   "source": [
    "%nbdev_default_export activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations functions.\n",
    "\n",
    "> Activations functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions, forked from https://github.com/rwightman/pytorch-image-models/timm/models/layers/activations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mish: Self Regularized  \n",
    "Non-Monotonic Activation Function  \n",
    "https://github.com/digantamisra98/Mish  \n",
    "fastai forum discussion https://forums.fast.ai/t/meet-mish-new-activation-function-possible-successor-to-relu  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# forked from https://github.com/rwightman/pytorch-image-models/timm/models/layers/activations.py\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "_USE_MEM_EFFICIENT_ISH = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "if _USE_MEM_EFFICIENT_ISH:\n",
    "    # This version reduces memory overhead of Swish during training by\n",
    "    # recomputing torch.sigmoid(x) in backward instead of saving it.\n",
    "    @torch.jit.script\n",
    "    def swish_jit_fwd(x):\n",
    "        return x.mul(torch.sigmoid(x))\n",
    "\n",
    "\n",
    "    @torch.jit.script\n",
    "    def swish_jit_bwd(x, grad_output):\n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "        return grad_output * (x_sigmoid * (1 + x * (1 - x_sigmoid)))\n",
    "\n",
    "\n",
    "    class SwishJitAutoFn(torch.autograd.Function):\n",
    "        \"\"\" torch.jit.script optimised Swish\n",
    "        Inspired by conversation btw Jeremy Howard & Adam Pazske\n",
    "        https://twitter.com/jeremyphoward/status/1188251041835315200\n",
    "        \"\"\"\n",
    "\n",
    "        @staticmethod\n",
    "        def forward(ctx, x):\n",
    "            ctx.save_for_backward(x)\n",
    "            return swish_jit_fwd(x)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            x = ctx.saved_tensors[0]\n",
    "            return swish_jit_bwd(x, grad_output)\n",
    "\n",
    "\n",
    "    def swish(x, _inplace=False):\n",
    "        return SwishJitAutoFn.apply(x)\n",
    "else:\n",
    "    def swish(x, inplace=False):\n",
    "        \"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\n",
    "        \"\"\"\n",
    "        return x.mul_(x.sigmoid()) if inplace else x.mul(x.sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_hide\n",
    "if _USE_MEM_EFFICIENT_ISH:\n",
    "    # This version reduces memory overhead of Swish during training by\n",
    "    # recomputing torch.sigmoid(x) in backward instead of saving it.\n",
    "    @torch.jit.script\n",
    "    def mish_jit_fwd(x):\n",
    "        return x.mul(torch.tanh(F.softplus(x)))\n",
    "\n",
    "\n",
    "    @torch.jit.script\n",
    "    def mish_jit_bwd(x, grad_output):\n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "        x_tanh_sp = F.softplus(x).tanh()\n",
    "        return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n",
    "\n",
    "\n",
    "    class MishJitAutoFn(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, x):\n",
    "            ctx.save_for_backward(x)\n",
    "            return mish_jit_fwd(x)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            x = ctx.saved_tensors[0]\n",
    "            return mish_jit_bwd(x, grad_output)\n",
    "\n",
    "    def mish(x, _inplace=False):\n",
    "        return MishJitAutoFn.apply(x)\n",
    "\n",
    "else:\n",
    "    def mish(x, _inplace=False):\n",
    "        \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "        \"\"\"\n",
    "        return x.mul(F.softplus(x).tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "@torch.jit.script\n",
    "def mish_jit_fwd(x):\n",
    "    return x.mul(torch.tanh(F.softplus(x)))\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def mish_jit_bwd(x, grad_output):\n",
    "    x_sigmoid = torch.sigmoid(x)\n",
    "    x_tanh_sp = F.softplus(x).tanh()\n",
    "    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n",
    "\n",
    "\n",
    "class MishJitAutoFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return mish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        return mish_jit_bwd(x, grad_output)\n",
    "\n",
    "def mish(x, _inplace=False):\n",
    "    return MishJitAutoFn.apply(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super(Swish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return swish(x, self.inplace)\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super(Mish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return mish(x, self.inplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def sigmoid(x, inplace=False):\n",
    "    return x.sigmoid_() if inplace else x.sigmoid()\n",
    "\n",
    "\n",
    "# PyTorch has this, but not with a consistent inplace argmument interface\n",
    "class Sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super(Sigmoid, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.sigmoid_() if self.inplace else x.sigmoid()\n",
    "\n",
    "\n",
    "def tanh(x, inplace=False):\n",
    "    return x.tanh_() if inplace else x.tanh()\n",
    "\n",
    "\n",
    "# PyTorch has this, but not with a consistent inplace argmument interface\n",
    "class Tanh(nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super(Tanh, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.tanh_() if self.inplace else x.tanh()\n",
    "\n",
    "\n",
    "def hard_swish(x, inplace=False):\n",
    "    inner = F.relu6(x + 3.).div_(6.)\n",
    "    return x.mul_(inner) if inplace else x.mul(inner)\n",
    "\n",
    "\n",
    "class HardSwish(nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super(HardSwish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return hard_swish(x, self.inplace)\n",
    "\n",
    "\n",
    "def hard_sigmoid(x, inplace=False):\n",
    "    if inplace:\n",
    "        return x.add_(3.).clamp_(0., 6.).div_(6.)\n",
    "    else:\n",
    "        return F.relu6(x + 3.) / 6.\n",
    "\n",
    "\n",
    "class HardSigmoid(nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super(HardSigmoid, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return hard_sigmoid(x, self.inplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_hide\n",
    "act_fn = Swish(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_hide\n",
    "act_fn = Mish(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end\n",
    "model_constructor\n",
    "by ayasyrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_constructor.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_resnet.ipynb.\n",
      "Converted 03_xresnet.ipynb.\n",
      "Converted 04_Net.ipynb.\n",
      "Converted 05_Twist.ipynb.\n",
      "Converted 06_YaResNet.ipynb.\n",
      "Converted 07_MXResNet.ipynb.\n",
      "Converted 08_activations.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "%nbdev_hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
