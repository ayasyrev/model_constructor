{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T16:20:00.256061Z",
     "start_time": "2020-11-11T16:20:00.027667Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T16:20:01.430479Z",
     "start_time": "2020-11-11T16:20:01.424122Z"
    }
   },
   "outputs": [],
   "source": [
    "#default_exp activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations functions.\n",
    "\n",
    "> Activations functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions, forked from https://github.com/rwightman/pytorch-image-models/timm/models/layers/activations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mish: Self Regularized  \n",
    "Non-Monotonic Activation Function  \n",
    "https://github.com/digantamisra98/Mish  \n",
    "fastai forum discussion https://forums.fast.ai/t/meet-mish-new-activation-function-possible-successor-to-relu  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T16:20:04.565883Z",
     "start_time": "2020-11-11T16:20:04.071041Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# forked from https://github.com/rwightman/pytorch-image-models/timm/models/layers/activations.py\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T16:20:08.693991Z",
     "start_time": "2020-11-11T16:20:08.687244Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def mish(x, inplace: bool = False):\n",
    "    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    NOTE: I don't have a working inplace variant\n",
    "    \"\"\"\n",
    "    return x.mul(F.softplus(x).tanh())\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        \"\"\"NOTE: inplace variant not working \"\"\"\n",
    "        super(Mish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return mish(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MishJit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T16:20:15.683629Z",
     "start_time": "2020-11-11T16:20:15.639490Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.jit.script\n",
    "def mish_jit(x, _inplace: bool = False):\n",
    "    \"\"\"Jit version of Mish.\n",
    "    Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    \"\"\"\n",
    "    return x.mul(F.softplus(x).tanh())\n",
    "\n",
    "\n",
    "class MishJit(nn.Module):\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        \"\"\"Jit version of Mish.\n",
    "        Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\"\"\"\n",
    "        super(MishJit, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return mish_jit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MishJitMe - memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T16:20:24.777571Z",
     "start_time": "2020-11-11T16:20:24.765170Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.jit.script\n",
    "def mish_jit_fwd(x):\n",
    "    # return x.mul(torch.tanh(F.softplus(x)))\n",
    "    return x.mul(F.softplus(x).tanh())\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def mish_jit_bwd(x, grad_output):\n",
    "    x_sigmoid = torch.sigmoid(x)\n",
    "    x_tanh_sp = F.softplus(x).tanh()\n",
    "    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n",
    "\n",
    "\n",
    "class MishJitAutoFn(torch.autograd.Function):\n",
    "    \"\"\" Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    A memory efficient, jit scripted variant of Mish\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return mish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        return mish_jit_bwd(x, grad_output)\n",
    "\n",
    "\n",
    "def mish_me(x, inplace=False):\n",
    "    return MishJitAutoFn.apply(x)\n",
    "\n",
    "\n",
    "class MishMe(nn.Module):\n",
    "    \"\"\" Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    A memory efficient, jit scripted variant of Mish\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(MishMe, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return MishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardMishJit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T16:20:43.826063Z",
     "start_time": "2020-11-11T16:20:43.817902Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.jit.script\n",
    "def hard_mish_jit(x, inplace: bool = False):\n",
    "    \"\"\" Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (x + 2).clamp(min=0, max=2)\n",
    "\n",
    "\n",
    "class HardMishJit(nn.Module):\n",
    "    \"\"\" Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardMishJit, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return hard_mish_jit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardMishJitMe - memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T16:20:46.737592Z",
     "start_time": "2020-11-11T16:20:46.698069Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.jit.script\n",
    "def hard_mish_jit_fwd(x):\n",
    "    return 0.5 * x * (x + 2).clamp(min=0, max=2)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def hard_mish_jit_bwd(x, grad_output):\n",
    "    m = torch.ones_like(x) * (x >= -2.)\n",
    "    m = torch.where((x >= -2.) & (x <= 0.), x + 1., m)\n",
    "    return grad_output * m\n",
    "\n",
    "\n",
    "class HardMishJitAutoFn(torch.autograd.Function):\n",
    "    \"\"\" A memory efficient, jit scripted variant of Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return hard_mish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        return hard_mish_jit_bwd(x, grad_output)\n",
    "\n",
    "\n",
    "def hard_mish_me(x, inplace: bool = False):\n",
    "    return HardMishJitAutoFn.apply(x)\n",
    "\n",
    "\n",
    "class HardMishMe(nn.Module):\n",
    "    \"\"\" A memory efficient, jit scripted variant of Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardMishMe, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return HardMishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T16:20:52.980532Z",
     "start_time": "2020-11-11T16:20:52.975600Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "act_fn = Mish(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end\n",
    "model_constructor\n",
    "by ayasyrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T16:24:49.841271Z",
     "start_time": "2020-11-11T16:24:49.167581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_Net.ipynb.\n",
      "Converted 01_activations.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 03_MXResNet.ipynb.\n",
      "Converted 04_YaResNet.ipynb.\n",
      "Converted 05_Twist.ipynb.\n",
      "Converted 10_base_constructor.ipynb.\n",
      "Converted 11_xresnet.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
