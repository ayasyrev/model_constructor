{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T07:39:27.761520Z",
     "start_time": "2020-09-19T07:39:27.123512Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T07:39:27.766214Z",
     "start_time": "2020-09-19T07:39:27.762895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells will be exported to model_constructor.activations,\n",
      "unless a different module is specified after an export flag: `%nbdev_export special.module`\n"
     ]
    }
   ],
   "source": [
    "%nbdev_default_export activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations functions.\n",
    "\n",
    "> Activations functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions, forked from https://github.com/rwightman/pytorch-image-models/timm/models/layers/activations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mish: Self Regularized  \n",
    "Non-Monotonic Activation Function  \n",
    "https://github.com/digantamisra98/Mish  \n",
    "fastai forum discussion https://forums.fast.ai/t/meet-mish-new-activation-function-possible-successor-to-relu  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T07:39:30.846492Z",
     "start_time": "2020-09-19T07:39:30.492725Z"
    }
   },
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# forked from https://github.com/rwightman/pytorch-image-models/timm/models/layers/activations.py\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T07:39:52.285184Z",
     "start_time": "2020-09-19T07:39:52.258095Z"
    }
   },
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def swish(x, inplace: bool = False):\n",
    "    \"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    return x.mul_(x.sigmoid()) if inplace else x.mul(x.sigmoid())\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(Swish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return swish(x, self.inplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SwishJit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T07:59:58.190958Z",
     "start_time": "2020-09-19T07:59:58.181329Z"
    }
   },
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "@torch.jit.script\n",
    "def swish_jit(x, inplace: bool = False):\n",
    "    \"\"\"Jit version of Swish.\n",
    "    Swish- Described in: https://arxiv.org/abs/1710.05941\n",
    "    \"\"\"\n",
    "    return x.mul(x.sigmoid())\n",
    "\n",
    "class SwishJit(nn.Module):\n",
    "    \"\"\"Jit version of Swish. \n",
    "    Swish - Described in: https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(SwishJit, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return swish_jit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SwishJitMe - memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:06:09.243621Z",
     "start_time": "2020-09-19T08:06:09.204597Z"
    }
   },
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "\n",
    "@torch.jit.script\n",
    "def swish_jit_bwd(x, grad_output):\n",
    "    x_sigmoid = torch.sigmoid(x)\n",
    "    return grad_output * (x_sigmoid * (1 + x * (1 - x_sigmoid)))\n",
    "\n",
    "\n",
    "class SwishJitAutoFn(torch.autograd.Function):\n",
    "    \"\"\" torch.jit.script optimised Swish w/ memory-efficient checkpoint\n",
    "    Inspired by conversation btw Jeremy Howard & Adam Pazske\n",
    "    https://twitter.com/jeremyphoward/status/1188251041835315200\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return swish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        return swish_jit_bwd(x, grad_output)\n",
    "\n",
    "\n",
    "def swish_me(x, inplace=False):\n",
    "    return SwishJitAutoFn.apply(x)\n",
    "\n",
    "\n",
    "class SwishMe(nn.Module):\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(SwishMe, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return SwishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T07:48:01.044576Z",
     "start_time": "2020-09-19T07:48:01.036045Z"
    }
   },
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def mish(x, inplace: bool = False):\n",
    "    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    NOTE: I don't have a working inplace variant\n",
    "    \"\"\"\n",
    "    return x.mul(F.softplus(x).tanh())\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        \"\"\"NOTE: inplace variant not working \"\"\"\n",
    "        super(Mish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return mish(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MishJit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T07:59:31.493260Z",
     "start_time": "2020-09-19T07:59:31.464218Z"
    }
   },
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "@torch.jit.script\n",
    "def mish_jit(x, _inplace: bool = False):\n",
    "    \"\"\"Jit version of Mish. \n",
    "    Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    \"\"\"\n",
    "    return x.mul(F.softplus(x).tanh())\n",
    "\n",
    "class MishJit(nn.Module):\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        \"\"\"Jit version of Mish. \n",
    "        Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\"\"\"\n",
    "        super(MishJit, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return mish_jit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MishJitMe - memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:12:49.922152Z",
     "start_time": "2020-09-19T08:12:49.903635Z"
    }
   },
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "\n",
    "@torch.jit.script\n",
    "def mish_jit_fwd(x):\n",
    "#     return x.mul(torch.tanh(F.softplus(x)))\n",
    "    return x.mul(F.softplus(x).tanh())\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def mish_jit_bwd(x, grad_output):\n",
    "    x_sigmoid = torch.sigmoid(x)\n",
    "    x_tanh_sp = F.softplus(x).tanh()\n",
    "    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n",
    "\n",
    "\n",
    "class MishJitAutoFn(torch.autograd.Function):\n",
    "    \"\"\" Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    A memory efficient, jit scripted variant of Mish\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return mish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        return mish_jit_bwd(x, grad_output)\n",
    "\n",
    "\n",
    "def mish_me(x, inplace=False):\n",
    "    return MishJitAutoFn.apply(x)\n",
    "\n",
    "\n",
    "class MishMe(nn.Module):\n",
    "    \"\"\" Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    A memory efficient, jit scripted variant of Mish\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(MishMe, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return MishJitAutoFn.apply(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardSwish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T07:42:36.529178Z",
     "start_time": "2020-09-19T07:42:36.519813Z"
    }
   },
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def hard_swish(x, inplace: bool = False):\n",
    "    \"\"\"Hard swish activation function\"\"\"\n",
    "    inner = F.relu6(x + 3.).div_(6.)\n",
    "    return x.mul_(inner) if inplace else x.mul(inner)\n",
    "\n",
    "\n",
    "class HardSwish(nn.Module):\n",
    "    \"\"\"Hard swish activation function\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardSwish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return hard_swish(x, self.inplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardSwishJit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "@torch.jit.script\n",
    "def hard_swish_jit(x, inplace: bool = False):\n",
    "    # return x * (F.relu6(x + 3.) / 6)\n",
    "    return x * (x + 3).clamp(min=0, max=6).div(6.)  # clamp seems ever so slightly faster?\n",
    "\n",
    "\n",
    "class HardSwishJit(nn.Module):\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardSwishJit, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return hard_swish_jit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardSwishJitMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:12:29.059919Z",
     "start_time": "2020-09-19T08:12:29.044200Z"
    }
   },
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "\n",
    "@torch.jit.script\n",
    "def hard_swish_jit_fwd(x):\n",
    "    return x * (x + 3).clamp(min=0, max=6).div(6.)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def hard_swish_jit_bwd(x, grad_output):\n",
    "    m = torch.ones_like(x) * (x >= 3.)\n",
    "    m = torch.where((x >= -3.) & (x <= 3.),  x / 3. + .5, m)\n",
    "    return grad_output * m\n",
    "\n",
    "\n",
    "class HardSwishJitAutoFn(torch.autograd.Function):\n",
    "    \"\"\"A memory efficient, jit-scripted HardSwish activation\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return hard_swish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        return hard_swish_jit_bwd(x, grad_output)\n",
    "\n",
    "\n",
    "def hard_swish_me(x, inplace=False):\n",
    "    \"\"\"A memory efficient, jit-scripted HardSwish activation\"\"\"\n",
    "    return HardSwishJitAutoFn.apply(x)\n",
    "\n",
    "\n",
    "class HardSwishMe(nn.Module):\n",
    "    \"\"\"A memory efficient, jit-scripted HardSwish activation\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardSwishMe, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return HardSwishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardMish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T07:50:17.702309Z",
     "start_time": "2020-09-19T07:50:17.690530Z"
    }
   },
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def hard_mish(x, inplace: bool = False):\n",
    "    \"\"\" Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    if inplace:\n",
    "        return x.mul_(0.5 * (x + 2).clamp(min=0, max=2))\n",
    "    else:\n",
    "        return 0.5 * x * (x + 2).clamp(min=0, max=2)\n",
    "\n",
    "\n",
    "class HardMish(nn.Module):\n",
    "    \"\"\"Hard Mish, Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardMish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return hard_mish(x, self.inplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardMishJit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:01:27.116469Z",
     "start_time": "2020-09-19T08:01:27.106399Z"
    }
   },
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "@torch.jit.script\n",
    "def hard_mish_jit(x, inplace: bool = False):\n",
    "    \"\"\" Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (x + 2).clamp(min=0, max=2)\n",
    "\n",
    "\n",
    "class HardMishJit(nn.Module):\n",
    "    \"\"\" Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardMishJit, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return hard_mish_jit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardMishJitMe - memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "@torch.jit.script\n",
    "def hard_mish_jit_fwd(x):\n",
    "    return 0.5 * x * (x + 2).clamp(min=0, max=2)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def hard_mish_jit_bwd(x, grad_output):\n",
    "    m = torch.ones_like(x) * (x >= -2.)\n",
    "    m = torch.where((x >= -2.) & (x <= 0.), x + 1., m)\n",
    "    return grad_output * m\n",
    "\n",
    "\n",
    "class HardMishJitAutoFn(torch.autograd.Function):\n",
    "    \"\"\" A memory efficient, jit scripted variant of Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return hard_mish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        return hard_mish_jit_bwd(x, grad_output)\n",
    "\n",
    "\n",
    "def hard_mish_me(x, inplace: bool = False):\n",
    "    return HardMishJitAutoFn.apply(x)\n",
    "\n",
    "\n",
    "class HardMishMe(nn.Module):\n",
    "    \"\"\" A memory efficient, jit scripted variant of Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardMishMe, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return HardMishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_hide\n",
    "act_fn = Swish(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_hide\n",
    "act_fn = Mish(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end\n",
    "model_constructor\n",
    "by ayasyrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:13:09.610451Z",
     "start_time": "2020-09-19T08:13:09.045551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_Net.ipynb.\n",
      "Converted 01_activations.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 03_MXResNet.ipynb.\n",
      "Converted 04_YaResNet.ipynb.\n",
      "Converted 05_Twist.ipynb.\n",
      "Converted 10_base_constructor.ipynb.\n",
      "Converted 11_xresnet.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "%nbdev_hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
