---

title: Activations functions.


keywords:  model constructor
sidebar: home_sidebar

summary: "Activations functions.  Set of act_fn."
description: "Activations functions.  Set of act_fn."
nb_path: "Nbs/01_activations.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: Nbs/01_activations.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Activation functions, forked from <a href="https://github.com/rwightman/pytorch-image-models/timm/models/layers/activations.py">https://github.com/rwightman/pytorch-image-models/timm/models/layers/activations.py</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Mish: Self Regularized<br>
Non-Monotonic Activation Function<br>
<a href="https://github.com/digantamisra98/Mish">https://github.com/digantamisra98/Mish</a><br>
fastai forum discussion <a href="https://forums.fast.ai/t/meet-mish-new-activation-function-possible-successor-to-relu">https://forums.fast.ai/t/meet-mish-new-activation-function-possible-successor-to-relu</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Mish is in Pytorch from version 1.9. Use this version!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Mish,-MishJit,-MishJitMe---memory-efficient.">Mish, MishJit, MishJitMe - memory-efficient.<a class="anchor-link" href="#Mish,-MishJit,-MishJitMe---memory-efficient."> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">model_constructor.activations</span> <span class="kn">import</span> <span class="n">mish</span><span class="p">,</span> <span class="n">Mish</span><span class="p">,</span> <span class="n">MishJit</span><span class="p">,</span> <span class="n">MishMe</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="mish" class="doc_header"><code>mish</code><a href="https://github.com/ayasyrev/model_constructor/tree/master/model_constructor/activations.py#L13" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>mish</code>(<strong><code>x</code></strong>, <strong><code>inplace</code></strong>:<code>bool</code>=<em><code>False</code></em>)</p>
</blockquote>
<p>Mish: A Self Regularized Non-Monotonic Neural Activation Function - <a href="https://arxiv.org/abs/1908.08681">https://arxiv.org/abs/1908.08681</a>
NOTE: I don't have a working inplace variant</p>

</div>

</div>

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Mish" class="doc_header"><code>class</code> <code>Mish</code><a href="torch/nn/modules/activation.py#L401" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Mish</code>(<strong><code>inplace</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Applies the Mish function, element-wise.
Mish: A Self Regularized Non-Monotonic Neural Activation Function.</p>
<p>.. math::
    \text{Mish}(x) = x * \text{Tanh}(\text{Softplus}(x))</p>
<p>.. note::
    See <code>Mish: A Self Regularized Non-Monotonic Neural Activation Function &lt;https://arxiv.org/abs/1908.08681&gt;</code>_</p>
<p>Shape:</p>

<pre><code>- Input: :math:`(N, *)` where `*` means, any number of additional
  dimensions
- Output: :math:`(N, *)`, same shape as the input

</code></pre>
<p>Examples::</p>

<pre><code>&gt;&gt;&gt; m = nn.Mish()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; output = m(input)</code></pre>

</div>

</div>

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MishJit" class="doc_header"><code>class</code> <code>MishJit</code><a href="https://github.com/ayasyrev/model_constructor/tree/master/model_constructor/activations.py#L40" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MishJit</code>(<strong><code>inplace</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MishMe" class="doc_header"><code>class</code> <code>MishMe</code><a href="https://github.com/ayasyrev/model_constructor/tree/master/model_constructor/activations.py#L81" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MishMe</code>(<strong><code>inplace</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Mish: A Self Regularized Non-Monotonic Neural Activation Function - <a href="https://arxiv.org/abs/1908.08681">https://arxiv.org/abs/1908.08681</a>
A memory efficient, jit scripted variant of Mish</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="HardMishJit,-HardMishJitMe---memory-efficient.">HardMishJit, HardMishJitMe - memory efficient.<a class="anchor-link" href="#HardMishJit,-HardMishJitMe---memory-efficient."> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">model_constructor.activations</span> <span class="kn">import</span> <span class="n">HardMishJit</span><span class="p">,</span> <span class="n">HardMishMe</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="HardMishJit" class="doc_header"><code>class</code> <code>HardMishJit</code><a href="https://github.com/ayasyrev/model_constructor/tree/master/model_constructor/activations.py#L100" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>HardMishJit</code>(<strong><code>inplace</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Hard Mish
Experimental, based on notes by Mish author Diganta Misra at
  <a href="https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md">https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md</a></p>

</div>

</div>

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="HardMishMe" class="doc_header"><code>class</code> <code>HardMishMe</code><a href="https://github.com/ayasyrev/model_constructor/tree/master/model_constructor/activations.py#L144" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>HardMishMe</code>(<strong><code>inplace</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>A memory efficient, jit scripted variant of Hard Mish
Experimental, based on notes by Mish author Diganta Misra at
  <a href="https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md">https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="end">end<a class="anchor-link" href="#end"> </a></h1><p>model_constructor
by ayasyrev</p>

</div>
</div>
</div>
</div>
 

