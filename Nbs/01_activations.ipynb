{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Activations functions.\n",
    "\n",
    "> Activations functions.  Set of act_fn."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Activation functions, forked from https://github.com/rwightman/pytorch-image-models/timm/models/layers/activations.py"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mish: Self Regularized  \n",
    "Non-Monotonic Activation Function  \n",
    "https://github.com/digantamisra98/Mish  \n",
    "fastai forum discussion https://forums.fast.ai/t/meet-mish-new-activation-function-possible-successor-to-relu  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# hide\n",
    "# forked from https://github.com/rwightman/pytorch-image-models/timm/models/layers/activations.py\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mish"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def mish(x, inplace: bool = False):\n",
    "    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    NOTE: I don't have a working inplace variant\n",
    "    \"\"\"\n",
    "    return x.mul(F.softplus(x).tanh())\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        \"\"\"NOTE: inplace variant not working \"\"\"\n",
    "        super(Mish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return mish(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MishJit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@torch.jit.script\n",
    "def mish_jit(x, _inplace: bool = False):\n",
    "    \"\"\"Jit version of Mish.\n",
    "    Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    \"\"\"\n",
    "    return x.mul(F.softplus(x).tanh())\n",
    "\n",
    "\n",
    "class MishJit(nn.Module):\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        \"\"\"Jit version of Mish.\n",
    "        Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\"\"\"\n",
    "        super(MishJit, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return mish_jit(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MishJitMe - memory-efficient."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@torch.jit.script\n",
    "def mish_jit_fwd(x):\n",
    "    # return x.mul(torch.tanh(F.softplus(x)))\n",
    "    return x.mul(F.softplus(x).tanh())\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def mish_jit_bwd(x, grad_output):\n",
    "    x_sigmoid = torch.sigmoid(x)\n",
    "    x_tanh_sp = F.softplus(x).tanh()\n",
    "    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n",
    "\n",
    "\n",
    "class MishJitAutoFn(torch.autograd.Function):\n",
    "    \"\"\" Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    A memory efficient, jit scripted variant of Mish\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return mish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        return mish_jit_bwd(x, grad_output)\n",
    "\n",
    "\n",
    "def mish_me(x, inplace=False):\n",
    "    return MishJitAutoFn.apply(x)\n",
    "\n",
    "\n",
    "class MishMe(nn.Module):\n",
    "    \"\"\" Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n",
    "    A memory efficient, jit scripted variant of Mish\"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(MishMe, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return MishJitAutoFn.apply(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HardMishJit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@torch.jit.script\n",
    "def hard_mish_jit(x, inplace: bool = False):\n",
    "    \"\"\" Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (x + 2).clamp(min=0, max=2)\n",
    "\n",
    "\n",
    "class HardMishJit(nn.Module):\n",
    "    \"\"\" Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardMishJit, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return hard_mish_jit(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HardMishJitMe - memory efficient."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@torch.jit.script\n",
    "def hard_mish_jit_fwd(x):\n",
    "    return 0.5 * x * (x + 2).clamp(min=0, max=2)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def hard_mish_jit_bwd(x, grad_output):\n",
    "    m = torch.ones_like(x) * (x >= -2.)\n",
    "    m = torch.where((x >= -2.) & (x <= 0.), x + 1., m)\n",
    "    return grad_output * m\n",
    "\n",
    "\n",
    "class HardMishJitAutoFn(torch.autograd.Function):\n",
    "    \"\"\" A memory efficient, jit scripted variant of Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return hard_mish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        return hard_mish_jit_bwd(x, grad_output)\n",
    "\n",
    "\n",
    "def hard_mish_me(x, inplace: bool = False):\n",
    "    return HardMishJitAutoFn.apply(x)\n",
    "\n",
    "\n",
    "class HardMishMe(nn.Module):\n",
    "    \"\"\" A memory efficient, jit scripted variant of Hard Mish\n",
    "    Experimental, based on notes by Mish author Diganta Misra at\n",
    "      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n",
    "    \"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(HardMishMe, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return HardMishJitAutoFn.apply(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#hide\n",
    "act_fn = Mish(inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# end\n",
    "model_constructor\n",
    "by ayasyrev"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}